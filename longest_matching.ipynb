{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Pnzy-C1IQVcH"},"outputs":[],"source":["import io\n","import re\n","import unicodedata as ud\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4hvI6AnQVcI"},"outputs":[],"source":["easy_sentence = io.open(\"./data/easy_sentence.txt\", encoding=\"utf-8\").readlines()\n","hard_sentence = io.open(\"./data/hard_sentence.txt\", encoding=\"utf-8\").readlines()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nMPvrQMJQVcJ","outputId":"89449ee1-d3d0-42d5-ae03-39e1545faeb5"},"outputs":[{"data":{"text/plain":["['thành phố washington có một kiến trúc rất đa dạng\\n',\n"," 'tuy nhiên vì gặp nhiều khó khăn trong cuộc sống ông dần trở nên khó tính\\n',\n"," 'khí hậu hồng kông thuộc kiểu cận nhiệt đới và chịu ảnh hưởng của gió mùa\\n',\n"," 'khoảng hơn 70 bề mặt trái đất được bao phủ bởi các đại dương nước mặn phần còn lại là các lục địa và các đảo\\n',\n"," 'đà lạt là thành phố trực thuộc tỉnh lâm đồng nằm trên cao nguyên lâm viên thuộc vùng tây nguyên việt nam\\n']"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data_sentence = easy_sentence\n","data_sentence.extend(hard_sentence)\n","data_sentence[:5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GlxQo1tvQVcJ"},"outputs":[],"source":["def syllablize(sentence):\n","    word = '\\w+'\n","    non_word = '[^\\w\\s]'\n","    digits = '\\d+([\\.,_]\\d+)+'\n","    \n","    patterns = []\n","    patterns.extend([word, non_word, digits])\n","    patterns = f\"({'|'.join(patterns)})\"\n","    \n","    sentence = ud.normalize('NFC', sentence)\n","    tokens = re.findall(patterns, sentence, re.UNICODE)\n","    return [token[0] for token in tokens]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXiUjswaQVcK"},"outputs":[],"source":["def load_n_grams(path):\n","    with open(path, encoding='utf8') as f:\n","        words = f.read().splitlines() \n","    return words"]},{"cell_type":"markdown","metadata":{"id":"qN2_4yqHQVcK"},"source":["# Longest Matching"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xtkp2wEzQVcK"},"outputs":[],"source":["def longest_matching(sentence, bi_grams, tri_grams):\n","    syllables = syllablize(sentence)\n","    syl_len = len(syllables)\n","\n","    curr_id = 0\n","    word_list = []\n","    done = False\n","    \n","    while (curr_id < syl_len) and (not done):\n","        curr_word = syllables[curr_id]\n","        if curr_id >= syl_len - 1:\n","            word_list.append(curr_word)\n","            done = True\n","        else:\n","            next_word = syllables[curr_id + 1]\n","            pair_word = ' '.join([curr_word.lower(), next_word.lower()])\n","            if curr_id >= (syl_len - 2):\n","                if pair_word in bi_grams:\n","                    word_list.append('_'.join([curr_word, next_word]))\n","                    curr_id += 2\n","                else:\n","                    word_list.append(curr_word)\n","                    curr_id += 1\n","            else:\n","                next_next_word = syllables[curr_id + 2]\n","                triple_word = ' '.join([pair_word, next_next_word.lower()])\n","                if triple_word in tri_grams:\n","                    word_list.append('_'.join([curr_word, next_word, next_next_word]))\n","                    curr_id += 3\n","                elif pair_word in bi_grams:\n","                    word_list.append('_'.join([curr_word, next_word]))\n","                    curr_id += 2\n","                else:\n","                    word_list.append(curr_word)\n","                    curr_id += 1\n","    return word_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djomOxMjQVcK"},"outputs":[],"source":["bi_grams = load_n_grams('./Vocab/vocab_bi_gram.txt')\n","tri_grams = load_n_grams('./Vocab/vocab_tri_gram.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnuC06qPQVcK","outputId":"3f558d3c-ef00-495a-fc53-86f767e6a22c"},"outputs":[{"data":{"text/plain":["['thành_phố washington có một kiến_trúc rất đa_dạng',\n"," 'tuy_nhiên vì gặp nhiều khó_khăn trong cuộc_sống ông dần trở_nên khó_tính',\n"," 'khí_hậu hồng_kông thuộc kiểu cận nhiệt_đới và chịu ảnh_hưởng của gió mùa']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["with open('Token/longest_matching_tokens.txt', 'w', encoding='utf-8') as f:\n","    longest_matching_sentences = []\n","    for sentence in data_sentence:\n","        word_list = longest_matching(sentence, bi_grams, tri_grams)\n","        longest_matching_sentences.append(' '.join(word_list))\n","        f.write(' '.join(word_list) + '\\n')\n","longest_matching_sentences[0:3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUAA7culQVcL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVVTRQUSQVcL","outputId":"875ef3c7-7c28-414a-9c5a-c91e8e0f6ac5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Số lượng từ ghép khi tách từ bằng thuật toán Longest Matching: 224\n"]}],"source":["count_longest_matching_compounds = 0\n","for sentence in longest_matching_sentences:\n","    for word in sentence.split():\n","        if '_' in word: count_longest_matching_compounds += 1\n","print('Số lượng từ ghép khi tách từ bằng thuật toán Longest Matching:', count_longest_matching_compounds)"]},{"cell_type":"markdown","metadata":{"id":"KRY0VCj0QVcL"},"source":["# VnCoreNLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-GREJ3tQVcL"},"outputs":[],"source":["import py_vncorenlp\n","model = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\", \"pos\"], save_dir='./')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGcJpat8QVcL","outputId":"eb288b9f-38b7-4181-8947-a343dbef4dc1"},"outputs":[{"data":{"text/plain":["['thành_phố washington có một kiến_trúc rất đa_dạng',\n"," 'tuy_nhiên vì gặp nhiều khó_khăn trong cuộc_sống ông dần trở_nên khó_tính',\n"," 'khí_hậu hồng_kông thuộc kiểu cận_nhiệt_đới và chịu ảnh_hưởng của gió_mùa']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["with open('Token/vncore_tokens.txt', 'w', encoding='utf-8') as f:\n","    vncore_sentences = []\n","    for sentence in data_sentence:\n","        words = model.word_segment(sentence)[0]\n","        vncore_sentences.append(words)\n","        f.write(words + '\\n')\n","vncore_sentences[0:3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1K_R2YI4QVcM","outputId":"e099b468-1af9-4c3f-e1c1-e0cceb7ccc9a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Số lượng từ ghép khi tách từ bằng thư viện VnCoreNLP: 231\n"]}],"source":["count_vncore_compounds = 0\n","for sentence in vncore_sentences:\n","    for word in sentence.split():\n","        if '_' in word: count_vncore_compounds += 1\n","print('Số lượng từ ghép khi tách từ bằng thư viện VnCoreNLP:', count_vncore_compounds)"]},{"cell_type":"markdown","metadata":{"id":"GCpWLTTcQVcM"},"source":["# Gold Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxnHVj5_QVcM"},"outputs":[],"source":["gold_sentence = io.open(\"./Data/gold_sentence.txt\", encoding=\"utf-8\").readlines()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lay8obPCQVcM","outputId":"fb07ba41-80ae-44cd-c138-a871a9f1dcfb"},"outputs":[{"data":{"text/plain":["['thành_phố washington có một kiến_trúc rất đa_dạng',\n"," 'tuy_nhiên vì gặp nhiều khó_khăn trong cuộc_sống ông dần trở_nên khó_tính',\n"," 'khí_hậu hồng_kông thuộc kiểu cận_nhiệt_đới và chịu ảnh_hưởng của gió_mùa']"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["manual_tokenize_sentences = []\n","for sentence in gold_sentence:\n","    if sentence != '\\n': \n","        manual_tokenize_sentences.append(sentence.strip())\n","manual_tokenize_sentences[0:3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KH1VJiKWQVcN","outputId":"adf0e2db-6609-4184-834a-fe06792b1bb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Số lượng từ ghép khi tách từ thủ công: 255\n"]}],"source":["count_manual_tokenize_compounds = 0\n","for sentence in manual_tokenize_sentences:\n","    for word in sentence.split():\n","        if '_' in word: count_manual_tokenize_compounds += 1\n","print('Số lượng từ ghép khi tách từ thủ công:', count_manual_tokenize_compounds)"]},{"cell_type":"markdown","metadata":{"id":"YklX7RzgQVcN"},"source":["# Đánh giá"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0Q63i2hQVcN"},"outputs":[],"source":["def count_correct_words(pred, source, n_grams=3):\n","    pred_words = pred.split()\n","    source_words = source.split()\n","    \n","    total_true, tp = 0, 0\n","    total_errors, fp = 0, 0\n","    \n","    idx = 0\n","    while idx < len(pred_words):\n","        if pred_words[idx] not in source_words[idx:(idx + n_grams)]: \n","            if '_' in pred_words[idx]: fp += 1\n","            del pred_words[idx]\n","            total_errors += 1\n","        else: idx += 1\n","    \n","    idx = 0\n","    while idx < len(source_words):\n","        if source_words[idx] not in pred_words[idx:(idx + n_grams)]: \n","            del source_words[idx]\n","        else: idx += 1\n","    \n","    if len(pred_words) < len(source_words): words = pred_words\n","    else: words = source_words\n","    \n","    for idx in range (len(words)):\n","        if pred_words[idx] == source_words[idx]:\n","            if '_' in pred_words[idx]: tp += 1 \n","            total_true += 1\n","                    \n","    return total_true, total_errors, tp, fp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xO5vxU1JQVcO"},"outputs":[],"source":["def tokenize_evaluation(pred, source, n_grams=3):\n","    total_true = 0\n","    total_errors = 0\n","    total_words = 0\n","    \n","    pred_tp = 0\n","    pred_fp = 0\n","    \n","    for pred_sentence, source_sentence in zip(pred, source):\n","        total_words += len(source_sentence.split())\n","        if pred_sentence != source_sentence:\n","            true, error, tp, fp = count_correct_words(pred_sentence, source_sentence, n_grams)\n","            total_true += true \n","            total_errors += error\n","            pred_tp += tp\n","            pred_fp += fp\n","        else:\n","            for word in source_sentence.split():\n","                if '_' in word:\n","                    pred_tp += 1\n","                total_true += 1\n","                    \n","    accuracy = total_true / total_words\n","    precision = pred_tp / (pred_tp + pred_fp)\n","    recall = pred_tp / count_manual_tokenize_compounds\n","    f1 = 2 * precision * recall / (precision + recall)\n","    return {\n","        'Accuracy': accuracy, \n","        'Precision': precision,\n","        'Recall': recall,\n","        'F1': f1,\n","        'True Positive': pred_tp, \n","        'False Positive': pred_fp,\n","        'Total True': total_true, \n","        'Total Errors': total_errors\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0K6fHHRiQVcP","outputId":"6884ac9a-a32f-40d0-d7a8-ccc0874f1628"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Longest Matching</th>\n","      <th>VnCoreNLP</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Accuracy</th>\n","      <td>0.90625</td>\n","      <td>0.915761</td>\n","    </tr>\n","    <tr>\n","      <th>Precision</th>\n","      <td>0.950893</td>\n","      <td>0.943723</td>\n","    </tr>\n","    <tr>\n","      <th>Recall</th>\n","      <td>0.835294</td>\n","      <td>0.854902</td>\n","    </tr>\n","    <tr>\n","      <th>F1</th>\n","      <td>0.889353</td>\n","      <td>0.897119</td>\n","    </tr>\n","    <tr>\n","      <th>True Positive</th>\n","      <td>213</td>\n","      <td>218</td>\n","    </tr>\n","    <tr>\n","      <th>False Positive</th>\n","      <td>11</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>Total True</th>\n","      <td>667</td>\n","      <td>674</td>\n","    </tr>\n","    <tr>\n","      <th>Total Errors</th>\n","      <td>97</td>\n","      <td>79</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               Longest Matching VnCoreNLP\n","Accuracy                0.90625  0.915761\n","Precision              0.950893  0.943723\n","Recall                 0.835294  0.854902\n","F1                     0.889353  0.897119\n","True Positive               213       218\n","False Positive               11        13\n","Total True                  667       674\n","Total Errors                 97        79"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["longest_matching_evaluation = tokenize_evaluation(longest_matching_sentences, manual_tokenize_sentences)\n","vncore_evaluation = tokenize_evaluation(vncore_sentences, manual_tokenize_sentences)\n","pd.DataFrame(\n","    [longest_matching_evaluation, vncore_evaluation], \n","    index = ['Longest Matching', 'VnCoreNLP']\n",").astype(object).T"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}