{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679d5f32",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aebffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "data_path = \"./Data\"\n",
    "file_name = os.listdir(data_path)\n",
    "file_path = [os.path.join(data_path, name) for name in file_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509e7489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./Data\\\\easy.txt', './Data\\\\easy_clean.txt', './Data\\\\hard.txt', './Data\\\\hard_clean.txt']\n"
     ]
    }
   ],
   "source": [
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a1b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy = io.open(\"./Data/easy.txt\", encoding=\"utf-8\").readlines()\n",
    "hard = io.open(\"./Data/hard.txt\", encoding=\"utf-8\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3f69435",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = easy\n",
    "data.extend(hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adddb736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thành phố washington có một kiến trúc rất đa dạng\\n',\n",
       " 'tuy nhiên vì gặp nhiều khó khăn trong cuộc sống ông dần trở nên khó tính\\n',\n",
       " 'khí hậu hồng kông thuộc kiểu cận nhiệt đới và chịu ảnh hưởng của gió mùa\\n',\n",
       " 'khoảng hơn 70 bề mặt trái đất được bao phủ bởi các đại dương nước mặn phần còn lại là các lục địa và các đảo\\n',\n",
       " 'đà lạt là thành phố trực thuộc tỉnh lâm đồng nằm trên cao nguyên lâm viên thuộc vùng tây nguyên việt nam\\n',\n",
       " 'đổi đất đai lấy hạ tầng là một trong những việc phải làm trong bối cảnh hiện nay\\n',\n",
       " 'cuối tuần trước tôi về quê vì gia đình có đám\\n',\n",
       " 'những cơn gió căng tràng vi vu khắp núi rừng đà lạt khiến những quả hồng uống no nê những giọt sương mờ\\n',\n",
       " 'nhưng đó là chuyện đã qua\\n',\n",
       " 'biến đổi khí hậu đe doạ đến con người khi nó gây bất an lương thực khan hiếm nước lũ lụt nắng nóng cực đoan thiệt hại kinh tế và di cư\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00963a2",
   "metadata": {},
   "source": [
    "# Tách từ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed03d0",
   "metadata": {},
   "source": [
    "## Longest Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e640188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata as ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0963637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllablize(sentence):\n",
    "    word = '\\w+'\n",
    "    non_word = '[^\\w\\s]'\n",
    "    digits = '\\d+([\\.,_]\\d+)+'\n",
    "    \n",
    "    patterns = []\n",
    "    patterns.extend([word, non_word, digits])\n",
    "    patterns = f\"({'|'.join(patterns)})\"\n",
    "    \n",
    "    sentence = ud.normalize('NFC', sentence)\n",
    "    tokens = re.findall(patterns, sentence, re.UNICODE)\n",
    "    return [token[0] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8327241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_n_grams(path):\n",
    "    with open(path, encoding='utf8') as f:\n",
    "        words = f.read().splitlines() \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "458a0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_matching(sentence, bi_grams, tri_grams):\n",
    "    syllables = syllablize(sentence)\n",
    "    syl_len = len(syllables)\n",
    "    \n",
    "    curr_id = 0\n",
    "    word_list = []\n",
    "    done = False\n",
    "    \n",
    "    while (curr_id < syl_len) and (not done):\n",
    "        curr_word = syllables[curr_id]\n",
    "        if curr_id >= syl_len - 1:\n",
    "            word_list.append(curr_word)\n",
    "            done = True\n",
    "        else:\n",
    "            next_word = syllables[curr_id + 1]\n",
    "            pair_word = ' '.join([curr_word.lower(), next_word.lower()])\n",
    "            if curr_id >= (syl_len - 2):\n",
    "                if pair_word in bi_grams:\n",
    "                    word_list.append('_'.join([curr_word, next_word]))\n",
    "                    curr_id += 2\n",
    "                else:\n",
    "                    word_list.append(curr_word)\n",
    "                    curr_id += 1\n",
    "            else:\n",
    "                next_next_word = syllables[curr_id + 2]\n",
    "                triple_word = ' '.join([pair_word, next_next_word.lower()])\n",
    "                if triple_word in tri_grams:\n",
    "                    word_list.append('_'.join([curr_word, next_word, next_next_word]))\n",
    "                    curr_id += 3\n",
    "                elif pair_word in bi_grams:\n",
    "                    word_list.append('_'.join([curr_word, next_word]))\n",
    "                    curr_id += 2\n",
    "                else:\n",
    "                    word_list.append(curr_word)\n",
    "                    curr_id += 1\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0f9b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams = load_n_grams('./Vocab/vocab_bi_gram.txt')\n",
    "tri_grams = load_n_grams('./Vocab/vocab_tri_gram.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8bdd3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nhưng', 'sự', 'thực_hiện', 'vẫn', 'còn', 'chưa', 'phù_hợp']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_matching('nhưng sự thực hiện vẫn còn chưa phù hợp', bi_grams, tri_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd3d3756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thành_phố washington có một kiến_trúc rất đa_dạng',\n",
       " 'tuy_nhiên vì gặp nhiều khó_khăn trong cuộc_sống ông dần trở_nên khó_tính',\n",
       " 'khí_hậu hồng_kông thuộc kiểu cận nhiệt_đới và chịu ảnh_hưởng của gió mùa']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Token/longest_matching_tokens.txt', 'w', encoding='utf-8') as f:\n",
    "    longest_matching_sentences = []\n",
    "    for sentence in data:\n",
    "        word_list = longest_matching(sentence, bi_grams, tri_grams)\n",
    "        longest_matching_sentences.append(' '.join(word_list))\n",
    "        f.write(' '.join(word_list) + '\\n')\n",
    "longest_matching_sentences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75909fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ ghép khi tách từ bằng thuật toán Longest Matching: 215\n"
     ]
    }
   ],
   "source": [
    "count_longest_matching_compounds = 0\n",
    "for sentence in longest_matching_sentences:\n",
    "    for word in sentence.split():\n",
    "        if '_' in word: count_longest_matching_compounds += 1\n",
    "print('Số lượng từ ghép khi tách từ bằng thuật toán Longest Matching:', count_longest_matching_compounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28230a8",
   "metadata": {},
   "source": [
    "## VnCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1ec04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp\n",
    "model = py_vncorenlp.VnCoreNLP(annotators=[\"wseg\", \"pos\"], save_dir='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fd11403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nhưng', 'sự', 'thực_hiện', 'vẫn', 'còn', 'chưa', 'phù_hợp']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_segment('nhưng sự thực hiện vẫn còn chưa phù hợp')[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99c32788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thành_phố washington có một kiến_trúc rất đa_dạng',\n",
       " 'tuy_nhiên vì gặp nhiều khó_khăn trong cuộc_sống ông dần trở_nên khó_tính',\n",
       " 'khí_hậu hồng_kông thuộc kiểu cận_nhiệt_đới và chịu ảnh_hưởng của gió_mùa']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Token/vncore_tokens.txt', 'w', encoding='utf-8') as f:\n",
    "    vncore_sentences = []\n",
    "    for sentence in data:\n",
    "        words = model.word_segment(sentence)[0]\n",
    "        vncore_sentences.append(words)\n",
    "        f.write(words + '\\n')\n",
    "vncore_sentences[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "087d4981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng từ ghép khi tách từ bằng thư viện VnCoreNLP: 231\n"
     ]
    }
   ],
   "source": [
    "count_vncore_compounds = 0\n",
    "for sentence in vncore_sentences:\n",
    "    for word in sentence.split():\n",
    "        if '_' in word: count_vncore_compounds += 1\n",
    "print('Số lượng từ ghép khi tách từ bằng thư viện VnCoreNLP:', count_vncore_compounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de65b510",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e541d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "520bdb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(data, train_size=0.8, random_state=23, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
